\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Q-learning通俗讲解}
\subtitle{用生活例子理解Off-policy TD控制}
\author{强化学习笔记}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{引言}

Q-learning是强化学习中最重要和最常用的算法之一。本文用通俗易懂的方式，通过生活化的例子来解释Q-learning的核心概念和工作原理。

\section{什么是Q-learning？}

\subsection{通俗理解}

\textbf{Q-learning就像}：

\begin{quote}
\textbf{"看别人怎么做，但学习的是最优方式，即使别人没选最优动作，你也学习最优动作的价值"}
\end{quote}

\textbf{核心特点}：
\begin{itemize}
    \item \textbf{Off-policy}：可以使用任何策略收集数据，但学习的是最优策略
    \item \textbf{使用max操作}：学习最优动作的价值，而不是实际选择的动作的价值
    \item \textbf{直接学习最优策略}：不需要显式策略改进步骤
\end{itemize}

\subsection{与SARSA的对比}

\textbf{SARSA（On-policy）}：
\begin{quote}
"你实际怎么做，就学习这个方式的好坏"
\end{quote}

\textbf{Q-learning（Off-policy）}：
\begin{quote}
"看别人怎么做，但学习的是最优方式的好坏"
\end{quote}

\section{核心概念：Off-policy}

\subsection{行为策略 vs 目标策略}

\textbf{行为策略}（Behavior Policy）$b$：
\begin{itemize}
    \item 用于生成数据（选择动作与环境交互）的策略
    \item 通常是探索性的（如 $\varepsilon$-贪婪策略）
    \item 决定你\textbf{实际怎么做}
\end{itemize}

\textbf{目标策略}（Target Policy）$\pi_*$：
\begin{itemize}
    \item 正在学习的策略（最优策略）
    \item 决定你\textbf{想要学会的方式}
    \item 通常是贪婪策略
\end{itemize}

\textbf{Off-policy条件}：
\begin{equation}
b \neq \pi_*
\end{equation}

\textbf{通俗理解}：
\begin{itemize}
    \item 你实际怎么做 $\neq$ 你想要学会的方式
    \item 你可以用任何方式收集数据，但学习的是最优方式
\end{itemize}

\subsection{生活例子：看别人做菜来学习}

\textbf{场景}：你观察别人做菜，学习最优做菜方式。

\textbf{行为策略}：别人怎么做菜（如"放两勺盐"）
\begin{itemize}
    \item 别人实际选择了"放两勺盐"
    \item 这是别人使用的策略
\end{itemize}

\textbf{目标策略}：你想要学会的最优做菜方式（如"放一勺半盐"）
\begin{itemize}
    \item 你想要学会"应该放多少盐"（最优策略）
    \item 可能最优是"放一勺半盐"，而不是"两勺"
\end{itemize}

\textbf{Q-learning的特点}：
\begin{itemize}
    \item 你观察别人放了两勺盐（行为策略选择）
    \item 但你学习的是"最优盐量"（可能是"一勺半"）的价值
    \item 即使别人放了两勺，你学习的是最优盐量的价值
\end{itemize}

\section{Q-learning更新公式}

\subsection{公式}

\textbf{Q-learning更新公式}：
\begin{equation}
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]
\label{eq:qlearning}
\end{equation}

\textbf{关键组成部分}：
\begin{itemize}
    \item $Q(S_t, A_t)$：当前状态-动作对的价值估计
    \item $R_{t+1}$：立即奖励
    \item $\max_{a} Q(S_{t+1}, a)$：下一状态所有动作中的最大价值（\textbf{关键！}）
    \item $\alpha$：学习率
    \item $\gamma$：折扣因子
\end{itemize}

\subsection{关键点：使用max操作}

\textbf{关键区别}：

\textbf{SARSA}：
\begin{equation}
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\end{equation}
\begin{itemize}
    \item 使用实际选择的动作 $A_{t+1}$ 的价值
    \item $A_{t+1}$ 由当前策略选择
\end{itemize}

\textbf{Q-learning}：
\begin{equation}
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{equation}
\begin{itemize}
    \item 使用所有动作中的最大价值 $\max_{a} Q(S_{t+1}, a)$
    \item 不关心实际选择了什么动作
    \item 学习的是最优动作的价值
\end{itemize}

\subsection{通俗解释}

\textbf{类比：学做菜}

\textbf{场景}：在状态"菜还没放盐"，你执行动作"放一勺盐"，转移到状态"菜已经放了一勺盐"。

\textbf{假设}：
\begin{itemize}
    \item $Q(\text{菜还没放盐}, \text{放一勺盐}) = 5$
    \item $Q(\text{菜已经放了一勺盐}, \text{放一勺盐}) = 3$
    \item $Q(\text{菜已经放了一勺盐}, \text{放两勺盐}) = 8$（最优）
    \item $Q(\text{菜已经放了一勺盐}, \text{放三勺盐}) = 2$
    \item 实际选择的动作：$A_{t+1} = \text{放一勺盐}$（行为策略选择，用于探索）
\end{itemize}

\textbf{SARSA更新}：
\begin{itemize}
    \item 使用实际选择的动作"放一勺盐"的价值：$Q(S_{t+1}, A_{t+1}) = 3$
    \item 更新：$Q(\text{菜还没放盐}, \text{放一勺盐}) \gets 5 + \alpha [R + 0.9 \times 3 - 5]$
    \item 学习的是"实际选择的动作"的价值
\end{itemize}

\textbf{Q-learning更新}：
\begin{itemize}
    \item 使用所有动作中的最大价值：$\max_{a} Q(S_{t+1}, a) = \max\{3, 8, 2\} = 8$
    \item 更新：$Q(\text{菜还没放盐}, \text{放一勺盐}) \gets 5 + \alpha [R + 0.9 \times 8 - 5]$
    \item 学习的是"最优动作"（放两勺盐）的价值，即使实际选择了"放一勺盐"
\end{itemize}

\section{为什么使用max操作？}

\subsection{理论原因}

\textbf{目标}：学习最优动作价值函数 $q_*(s, a)$

\textbf{最优动作价值函数的贝尔曼方程}：
\begin{equation}
q_*(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q_*(s', a')\right]
\end{equation}

\textbf{关键}：
\begin{itemize}
    \item 最优动作价值函数使用 $\max_{a'} q_*(s', a')$
    \item Q-learning直接学习最优动作价值函数
    \item 因此必须使用 $\max$ 操作
\end{itemize}

\subsection{通俗原因}

\textbf{类比：学做菜}

\textbf{问题}：为什么即使别人选择了"放一勺盐"，你学习的是"放两勺盐"的价值？

\textbf{答案}：
\begin{itemize}
    \item 你的目标是学会"最优做菜方式"
    \item 即使别人选择了次优动作，你也要学习最优动作的价值
    \item 这样你才能知道"最优方式"是什么
    \item 如果学习实际选择的动作，你学到的可能是次优方式
\end{itemize}

\textbf{例子}：
\begin{itemize}
    \item 别人选择了"放一勺盐"（次优，价值3）
    \item 但最优是"放两勺盐"（价值8）
    \item 如果你学习"放一勺盐"的价值，你学到的就是次优方式
    \item 如果你学习"放两勺盐"的价值，你学到的就是最优方式
    \item 所以Q-learning使用 $\max$ 操作，学习最优动作的价值
\end{itemize}

\section{Q-learning算法流程}

\subsection{完整算法}

\textbf{初始化}：
\begin{itemize}
    \item 初始化 $Q(s, a)$（可以任意，通常设为0）
    \item 初始化行为策略 $b$（如 $\varepsilon$-贪婪策略）
\end{itemize}

\textbf{每个时间步}：
\begin{enumerate}
    \item 在状态 $S_t$，使用行为策略 $b$ 选择动作 $A_t$
    \begin{equation}
    A_t \sim b(\cdot|S_t)
    \end{equation}
    
    \item 执行动作 $A_t$，观察 $R_{t+1}, S_{t+1}$
    
    \item 在状态 $S_{t+1}$，使用行为策略 $b$ 选择动作 $A_{t+1}$（用于下一步，但更新时不使用）
    \begin{equation}
    A_{t+1} \sim b(\cdot|S_{t+1})
    \end{equation}
    
    \item 更新动作价值函数：
    \begin{equation}
    Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]
    \end{equation}
    
    \item \textbf{关键}：更新时使用 $\max_{a} Q(S_{t+1}, a)$，而不是 $Q(S_{t+1}, A_{t+1})$
    
    \item 更新行为策略 $b$（基于新的 $Q$，如 $\varepsilon$-贪婪策略）
\end{enumerate}

\textbf{提取最优策略}：
\begin{equation}
\pi_*(s) = \arg\max_{a} Q(s, a)
\end{equation}

\subsection{通俗例子：学做菜}

\textbf{场景}：学习如何做一道菜，需要决定放多少盐。

\textbf{状态}：
\begin{itemize}
    \item $s_1$：菜还没放盐
    \item $s_2$：菜已经放了一勺盐
    \item $s_3$：菜已经放了两勺盐
\end{itemize}

\textbf{动作}：
\begin{itemize}
    \item $a_1$：放一勺盐
    \item $a_2$：放两勺盐
    \item $a_3$：放三勺盐
\end{itemize}

\textbf{第1次做菜}：
\begin{enumerate}
    \item 在状态 $s_1$，使用行为策略 $b$（$\varepsilon$-贪婪）选择动作
    \item 你选择了 $a_2$（放两勺盐，用于探索）
    \item 执行动作，转移到状态 $s_3$，获得奖励 $R = 2$
    \item 在状态 $s_3$，使用行为策略 $b$ 选择动作 $a_1$（放一勺盐，实际选择的动作）
    \item 更新：
    \begin{align}
    Q(s_1, a_2) &\gets Q(s_1, a_2) + \alpha [R + \gamma \max_{a} Q(s_3, a) - Q(s_1, a_2)] \\
                &\gets 0 + 0.1 [2 + 0.9 \times \max\{Q(s_3, a_1), Q(s_3, a_2), Q(s_3, a_3)\} - 0]
    \end{align}
    \item 假设 $\max_{a} Q(s_3, a) = 5$（最优动作的价值）
    \item 更新：$Q(s_1, a_2) \gets 0 + 0.1 [2 + 0.9 \times 5 - 0] = 0.65$
    \item \textbf{关键}：即使实际选择了 $a_1$，你学习的是最优动作的价值（5）
\end{enumerate}

\textbf{第2次做菜}：
\begin{enumerate}
    \item 在状态 $s_1$，使用行为策略 $b$ 选择动作
    \item 你选择了 $a_1$（放一勺盐）
    \item 执行动作，转移到状态 $s_2$，获得奖励 $R = 1$
    \item 在状态 $s_2$，使用行为策略 $b$ 选择动作 $a_3$（放三勺盐，实际选择的动作）
    \item 更新：
    \begin{align}
    Q(s_1, a_1) &\gets Q(s_1, a_1) + \alpha [R + \gamma \max_{a} Q(s_2, a) - Q(s_1, a_1)] \\
                &\gets 0 + 0.1 [1 + 0.9 \times \max_{a} Q(s_2, a) - 0]
    \end{align}
    \item 假设 $\max_{a} Q(s_2, a) = 4$（最优动作的价值）
    \item 更新：$Q(s_1, a_1) \gets 0 + 0.1 [1 + 0.9 \times 4 - 0] = 0.46$
    \item \textbf{关键}：即使实际选择了 $a_3$，你学习的是最优动作的价值（4）
\end{enumerate}

\textbf{继续学习}：
\begin{itemize}
    \item 你继续做菜，根据结果更新 $Q$ 值
    \item 每次更新都使用最优动作的价值，而不是实际选择的动作的价值
    \item 最终，你学会了最优做菜方式
\end{itemize}

\section{Q-learning vs SARSA}

\subsection{核心区别}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{特征} & \textbf{SARSA} & \textbf{Q-learning} \\
\hline
\textbf{策略类型} & On-policy & Off-policy \\
\hline
\textbf{行为策略} & 当前策略 $\pi$ & 行为策略 $b$（$\varepsilon$-贪婪） \\
\hline
\textbf{目标策略} & 当前策略 $\pi$ & 最优策略 $\pi_*$ \\
\hline
\textbf{更新目标} & $Q(S_{t+1}, A_{t+1})$ & $\max_{a} Q(S_{t+1}, a)$ \\
\hline
\textbf{使用实际动作} & 是 & 否 \\
\hline
\textbf{学习目标} & 当前策略的价值函数 & 最优策略的价值函数 \\
\hline
\textbf{探索风险} & 考虑（保守） & 不考虑（激进） \\
\hline
\end{tabular}
\end{center}

\subsection{通俗对比}

\textbf{SARSA：边学边用}
\begin{itemize}
    \item 你实际怎么做，就学习这个方式的好坏
    \item 你实际选择了"放一勺盐"，就学习"放一勺盐"的价值
    \item 你考虑探索的风险（如果探索导致不好的结果，你会学到）
    \item 你学习的是"你当前怎么做"的价值
\end{itemize}

\textbf{Q-learning：看别人学最优}
\begin{itemize}
    \item 你观察别人怎么做，但学习的是最优方式的好坏
    \item 即使别人选择了"放一勺盐"，你学习的是"最优盐量"的价值
    \item 你不考虑探索的风险（总是学习最优动作的价值）
    \item 你学习的是"最优方式"的价值
\end{itemize}

\subsection{具体例子对比}

\textbf{场景}：在Gridworld中，智能体在状态 $s_5$，执行动作"上"，转移到状态 $s_2$。

\textbf{假设}：
\begin{itemize}
    \item $Q(s_2, \text{上}) = 0$
    \item $Q(s_2, \text{下}) = 0$
    \item $Q(s_2, \text{左}) = -0.1$
    \item $Q(s_2, \text{右}) = 0$
    \item 实际选择的动作：$A_{t+1} = \text{左}$（行为策略选择，用于探索）
\end{itemize}

\textbf{SARSA更新}：
\begin{equation}
Q(s_5, \text{上}) \gets Q(s_5, \text{上}) + \alpha [R + \gamma Q(s_2, \text{左}) - Q(s_5, \text{上})]
\end{equation}
\begin{itemize}
    \item 使用实际选择的动作"左"的价值：$Q(s_2, \text{左}) = -0.1$
    \item 学习的是"实际选择的动作"的价值
    \item 如果探索导致不好的结果，SARSA会学到这个风险
\end{itemize}

\textbf{Q-learning更新}：
\begin{equation}
Q(s_5, \text{上}) \gets Q(s_5, \text{上}) + \alpha [R + \gamma \max_{a} Q(s_2, a) - Q(s_5, \text{上})]
\end{equation}
\begin{itemize}
    \item 使用所有动作中的最大价值：$\max_{a} Q(s_2, a) = \max\{0, 0, -0.1, 0\} = 0$
    \item 学习的是"最优动作"（上、下、右）的价值
    \item 即使实际选择了"左"（不好的动作），也学习最优动作的价值
    \item 不考虑探索的风险
\end{itemize}

\section{Q-learning的优势}

\subsection{理论优势}

\textbf{1. 直接学习最优策略}
\begin{itemize}
    \item Q-learning直接学习最优动作价值函数 $q_*(s, a)$
    \item 不需要显式策略改进步骤
    \item 最终策略就是最优策略
\end{itemize}

\textbf{2. Off-policy学习}
\begin{itemize}
    \item 可以使用任何策略收集数据
    \item 可以从历史数据中学习
    \item 样本效率高
\end{itemize}

\textbf{3. 简单高效}
\begin{itemize}
    \item 算法简单，易于实现
    \item 更新规则清晰
    \item 收敛性好
\end{itemize}

\subsection{实际优势}

\textbf{1. 可以重用历史数据}
\begin{itemize}
    \item 可以使用之前收集的数据
    \item 不需要重新收集数据
    \item 样本效率高
\end{itemize}

\textbf{2. 可以并行学习}
\begin{itemize}
    \item 多个智能体可以共享经验
    \item 可以从其他智能体的经验中学习
    \item 加速学习过程
\end{itemize}

\textbf{3. 适合在线学习}
\begin{itemize}
    \item 可以立即更新
    \item 不需要等待回合结束
    \item 适合实时应用
\end{itemize}

\section{Q-learning的局限性}

\subsection{理论局限性}

\textbf{1. 不考虑探索风险}
\begin{itemize}
    \item Q-learning总是学习最优动作的价值
    \item 如果环境有风险（如悬崖），Q-learning可能不够保守
    \item SARSA在这种情况下可能更好
\end{itemize}

\textbf{2. 需要充分探索}
\begin{itemize}
    \item 需要探索所有状态-动作对
    \item 如果探索不足，可能无法找到最优策略
    \item 需要合适的探索策略（如 $\varepsilon$-贪婪）
\end{itemize}

\textbf{3. 函数逼近的挑战}
\begin{itemize}
    \item 在大状态空间中，需要使用函数逼近
    \item 函数逼近可能导致不收敛
    \item 需要特殊的技术（如经验回放、目标网络）
\end{itemize}

\subsection{实际局限性}

\textbf{1. 高估问题}
\begin{itemize}
    \item 使用 $\max$ 操作可能导致高估
    \item 需要技术来减少高估（如Double Q-learning）
\end{itemize}

\textbf{2. 样本效率}
\begin{itemize}
    \item 在某些情况下，样本效率可能不如On-policy方法
    \item 需要大量数据才能收敛
\end{itemize}

\section{总结}

\subsection{核心要点（通俗版）}

\begin{enumerate}
    \item \textbf{Q-learning}：看别人怎么做，但学习的是最优方式的好坏
    
    \item \textbf{Off-policy}：你实际怎么做 $\neq$ 你想要学会的方式
    
    \item \textbf{使用max操作}：学习最优动作的价值，而不是实际选择的动作的价值
    
    \item \textbf{直接学习最优策略}：不需要显式策略改进步骤
    
    \item \textbf{可以重用数据}：可以使用历史数据，样本效率高
\end{enumerate}

\subsection{关键公式（简化版）}

\textbf{Q-learning更新}：
\begin{equation}
\text{新价值} = \text{旧价值} + \alpha \times [\text{立即奖励} + \gamma \times \text{最优未来价值} - \text{旧价值}]
\end{equation}

\textbf{关键}：
\begin{equation}
\text{最优未来价值} = \max_{a} Q(\text{下一状态}, a)
\end{equation}

\textbf{提取最优策略}：
\begin{equation}
\text{最优动作} = \arg\max_{a} Q(\text{当前状态}, a)
\end{equation}

\subsection{记忆技巧}

\begin{itemize}
    \item \textbf{Q-learning}：看别人学，但学最优的
    \item \textbf{使用max}：总是学最优动作的价值
    \item \textbf{Off-policy}：实际做的 $\neq$ 想学的
    \item \textbf{直接最优}：不需要策略改进，直接学最优策略
    \item \textbf{可以重用}：可以用历史数据，效率高
\end{itemize}

\subsection{适用场景}

\textbf{Q-learning适合}：
\begin{itemize}
    \item 需要学习最优策略的问题
    \item 可以重用历史数据的情况
    \item 环境相对安全，探索风险不大的情况
    \item 需要在线学习的情况
\end{itemize}

\textbf{Q-learning不适合}：
\begin{itemize}
    \item 环境有高风险的情况（如悬崖问题）
    \item 需要保守策略的情况
    \item 探索成本很高的情况
\end{itemize}

\end{document}

