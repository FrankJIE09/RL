\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{tikz}
\usepackage{pgfplots}

\geometry{margin=2.5cm}

\title{Soft-Max 分布详解}
\author{}
\date{}

\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\newtheorem{proposition}{命题}
\newtheorem{example}{示例}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{引言}

Soft-Max 分布（也称为 Gibbs 分布或 Boltzmann 分布）是概率论和机器学习中的一个重要概念。它将一组实数（通常称为"分数"或"偏好"）转换为概率分布，使得较大的值对应较高的概率。Soft-Max 分布在分类问题、强化学习、神经网络等领域有广泛应用。

\section{定义与基本形式}

\subsection{基本定义}

\begin{definition}[Soft-Max 分布]
给定 $k$ 个实数 $z_1, z_2, \ldots, z_k$，soft-max 分布定义为：
\begin{equation}
\sigma_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}, \quad i = 1, 2, \ldots, k
\end{equation}
其中 $\sigma_i$ 表示第 $i$ 个元素的概率。
\end{definition}

\textbf{关键特性}：
\begin{enumerate}
    \item \textbf{归一化}：$\sum_{i=1}^k \sigma_i = 1$
    \item \textbf{非负性}：$\sigma_i \geq 0$ 对所有 $i$
    \item \textbf{单调性}：如果 $z_i > z_j$，则 $\sigma_i > \sigma_j$
\end{enumerate}

\subsection{向量形式}

用向量表示，设 $\bm{z} = (z_1, z_2, \ldots, z_k)^T$，则 soft-max 函数为：
\begin{equation}
\text{softmax}(\bm{z})_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}} = \frac{e^{z_i}}{e^{z_1} + e^{z_2} + \cdots + e^{z_k}}
\end{equation}

\subsection{在强化学习中的应用}

在强化学习的多臂老虎机问题中，soft-max 分布用于将动作偏好 $H_t(a)$ 转换为动作选择概率：
\begin{equation}
\pi_t(a) = \Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}
\end{equation}

\section{数学性质}

\subsection{平移不变性}

\begin{proposition}[平移不变性]
Soft-Max 分布具有平移不变性：对任意常数 $c$，有
\begin{equation}
\text{softmax}(\bm{z} + c\bm{1}) = \text{softmax}(\bm{z})
\end{equation}
其中 $\bm{1} = (1, 1, \ldots, 1)^T$ 是全1向量。
\end{proposition}

\textbf{证明}：
\begin{align}
\text{softmax}(\bm{z} + c\bm{1})_i &= \frac{e^{z_i + c}}{\sum_{j=1}^k e^{z_j + c}} \\
                                   &= \frac{e^c \cdot e^{z_i}}{e^c \cdot \sum_{j=1}^k e^{z_j}} \\
                                   &= \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}} \\
                                   &= \text{softmax}(\bm{z})_i
\end{align}

\textbf{意义}：这意味着 soft-max 只依赖于输入值的\textbf{相对大小}，而不是绝对值。这是 soft-max 的一个重要特性，使得我们可以对输入进行任意平移而不改变输出分布。

\subsection{尺度敏感性}

虽然 soft-max 对平移不敏感，但它对尺度（缩放）是敏感的。如果我们将所有输入乘以常数 $\beta$：
\begin{equation}
\text{softmax}(\beta \bm{z})_i = \frac{e^{\beta z_i}}{\sum_{j=1}^k e^{\beta z_j}}
\end{equation}

\textbf{极限行为}：
\begin{itemize}
    \item 当 $\beta \to 0$ 时，分布趋于均匀分布：$\lim_{\beta \to 0} \text{softmax}(\beta \bm{z})_i = \frac{1}{k}$
    \item 当 $\beta \to \infty$ 时，分布趋于确定性分布（one-hot）：
    \begin{equation}
    \lim_{\beta \to \infty} \text{softmax}(\beta \bm{z})_i = \begin{cases}
    1 & \text{如果 } z_i = \max_j z_j \\
    0 & \text{否则}
    \end{cases}
    \end{equation}
\end{itemize}

参数 $\beta$ 通常被称为\textbf{温度参数}（temperature parameter）。在机器学习中，有时会使用带温度参数的 soft-max：
\begin{equation}
\text{softmax}_\beta(\bm{z})_i = \frac{e^{\beta z_i}}{\sum_{j=1}^k e^{\beta z_j}}
\end{equation}

\subsection{可微性}

Soft-Max 函数是连续可微的，这对于基于梯度的优化算法非常重要。其梯度为：
\begin{equation}
\frac{\partial \text{softmax}(\bm{z})_i}{\partial z_j} = \begin{cases}
\text{softmax}(\bm{z})_i (1 - \text{softmax}(\bm{z})_i) & \text{如果 } i = j \\
-\text{softmax}(\bm{z})_i \cdot \text{softmax}(\bm{z})_j & \text{如果 } i \neq j
\end{cases}
\end{equation}

更紧凑的形式：
\begin{equation}
\frac{\partial \sigma_i}{\partial z_j} = \sigma_i (\delta_{ij} - \sigma_j)
\end{equation}
其中 $\delta_{ij}$ 是 Kronecker delta 函数。

\section{与相关分布的关系}

\subsection{与 Hard-Max 的关系}

\textbf{Hard-Max}（也称为 argmax）是选择最大值的索引：
\begin{equation}
\text{hardmax}(\bm{z})_i = \begin{cases}
1 & \text{如果 } i = \arg\max_j z_j \\
0 & \text{否则}
\end{cases}
\end{equation}

Soft-Max 可以看作是 Hard-Max 的"平滑"版本。当温度参数 $\beta \to \infty$ 时，soft-max 收敛到 hard-max。

\subsection{与 Sigmoid 函数的关系}

对于二分类情况（$k=2$），soft-max 等价于 sigmoid 函数。

设 $z_1$ 和 $z_2$ 是两个输入，则：
\begin{align}
\sigma_1 &= \frac{e^{z_1}}{e^{z_1} + e^{z_2}} \\
         &= \frac{1}{1 + e^{z_2 - z_1}} \\
         &= \text{sigmoid}(z_1 - z_2)
\end{align}

类似地：
\begin{equation}
\sigma_2 = \text{sigmoid}(z_2 - z_1) = 1 - \sigma_1
\end{equation}

因此，对于二分类，soft-max 本质上就是 sigmoid 函数。

\subsection{与 Log-Sum-Exp 的关系}

Soft-Max 的对数形式与 log-sum-exp 函数相关：
\begin{equation}
\log \text{softmax}(\bm{z})_i = z_i - \log\sum_{j=1}^k e^{z_j} = z_i - \text{LSE}(\bm{z})
\end{equation}

其中 $\text{LSE}(\bm{z}) = \log\sum_{j=1}^k e^{z_j}$ 是 log-sum-exp 函数。

这个关系在数值计算中很有用，因为 log-sum-exp 有数值稳定的实现方法。

\section{数值稳定性}

\subsection{溢出问题}

直接计算 $e^{z_i}$ 时，如果 $z_i$ 很大，可能会导致数值溢出。例如，在 Python 中：
\begin{verbatim}
>>> import numpy as np
>>> z = np.array([1000, 1001, 1002])
>>> np.exp(z)
array([inf, inf, inf])  # 溢出！
\end{verbatim}

\subsection{数值稳定的实现}

利用平移不变性，我们可以减去最大值来避免溢出：
\begin{equation}
\text{softmax}(\bm{z})_i = \frac{e^{z_i - \max_j z_j}}{\sum_{j=1}^k e^{z_j - \max_j z_j}}
\end{equation}

\textbf{算法步骤}：
\begin{enumerate}
    \item 计算 $m = \max_j z_j$
    \item 计算 $\tilde{z}_i = z_i - m$（对所有 $i$）
    \item 计算 $\text{softmax}(\bm{z})_i = \frac{e^{\tilde{z}_i}}{\sum_j e^{\tilde{z}_j}}$
\end{enumerate}

\textbf{Python 实现}：
\begin{verbatim}
def softmax_stable(z):
    z_shifted = z - np.max(z)
    exp_z = np.exp(z_shifted)
    return exp_z / np.sum(exp_z)
\end{verbatim}

\subsection{下溢问题}

虽然减去最大值解决了溢出问题，但如果 $z_i - \max_j z_j$ 很小（负值很大），$e^{z_i - \max_j z_j}$ 可能会下溢到 0。不过，由于我们只关心归一化后的结果，下溢通常不会造成问题（因为其他项也会相应调整）。

\section{应用场景}

\subsection{多分类问题}

在机器学习中，soft-max 常用于多分类问题的输出层。给定输入特征 $\bm{x}$ 和权重矩阵 $\bm{W}$，输出为：
\begin{equation}
\bm{z} = \bm{W}\bm{x} + \bm{b}
\end{equation}
然后通过 soft-max 转换为类别概率：
\begin{equation}
P(y = i | \bm{x}) = \text{softmax}(\bm{z})_i
\end{equation}

\subsection{强化学习中的策略表示}

在强化学习中，soft-max 用于将动作偏好转换为策略：
\begin{equation}
\pi(a | s) = \frac{e^{H(s, a)}}{\sum_{b \in \mathcal{A}} e^{H(s, b)}}
\end{equation}
其中 $H(s, a)$ 是状态 $s$ 下动作 $a$ 的偏好值。

\subsection{注意力机制}

在 Transformer 等模型中，soft-max 用于计算注意力权重：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\section{梯度计算}

\subsection{Soft-Max 的梯度}

对于 soft-max 函数 $\sigma_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$，其梯度为：
\begin{equation}
\frac{\partial \sigma_i}{\partial z_j} = \sigma_i (\delta_{ij} - \sigma_j)
\end{equation}

\textbf{证明}：
\begin{align}
\frac{\partial \sigma_i}{\partial z_j} &= \frac{\partial}{\partial z_j} \left[\frac{e^{z_i}}{\sum_k e^{z_k}}\right] \\
&= \frac{\delta_{ij} e^{z_i} \sum_k e^{z_k} - e^{z_i} e^{z_j}}{\left(\sum_k e^{z_k}\right)^2} \\
&= \frac{\delta_{ij} e^{z_i}}{\sum_k e^{z_k}} - \frac{e^{z_i}}{\sum_k e^{z_k}} \cdot \frac{e^{z_j}}{\sum_k e^{z_k}} \\
&= \sigma_i \delta_{ij} - \sigma_i \sigma_j \\
&= \sigma_i (\delta_{ij} - \sigma_j)
\end{align}

\subsection{交叉熵损失的梯度}

在分类问题中，常使用交叉熵损失：
\begin{equation}
L = -\sum_{i=1}^k y_i \log \sigma_i
\end{equation}
其中 $\bm{y}$ 是真实标签（one-hot 编码）。

结合 soft-max，损失函数关于 $\bm{z}$ 的梯度为：
\begin{align}
\frac{\partial L}{\partial z_j} &= -\sum_{i=1}^k y_i \frac{1}{\sigma_i} \frac{\partial \sigma_i}{\partial z_j} \\
                                &= -\sum_{i=1}^k y_i \frac{1}{\sigma_i} \sigma_i (\delta_{ij} - \sigma_j) \\
                                &= -\sum_{i=1}^k y_i (\delta_{ij} - \sigma_j) \\
                                &= -y_j + \sigma_j \sum_{i=1}^k y_i \\
                                &= \sigma_j - y_j
\end{align}

这是一个非常简洁的结果：梯度就是预测概率与真实标签的差！

\section{示例与可视化}

\subsection{简单示例}

考虑三个输入值：$z_1 = 1, z_2 = 2, z_3 = 3$。

\textbf{计算过程}：
\begin{align}
e^{z_1} &= e^1 \approx 2.718 \\
e^{z_2} &= e^2 \approx 7.389 \\
e^{z_3} &= e^3 \approx 20.086 \\
\sum e^{z_i} &\approx 30.193
\end{align}

\textbf{Soft-Max 输出}：
\begin{align}
\sigma_1 &= \frac{2.718}{30.193} \approx 0.090 \\
\sigma_2 &= \frac{7.389}{30.193} \approx 0.245 \\
\sigma_3 &= \frac{20.086}{30.193} \approx 0.665
\end{align}

注意：$\sigma_1 + \sigma_2 + \sigma_3 = 1$，且 $\sigma_3 > \sigma_2 > \sigma_1$（因为 $z_3 > z_2 > z_1$）。

\subsection{温度参数的影响}

下图展示了不同温度参数对 soft-max 分布的影响（假设 $z = [1, 2, 3]$）：

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$\beta$ & $\sigma_1$ & $\sigma_2$ & $\sigma_3$ \\
\hline
0.1 & 0.333 & 0.333 & 0.333 \\
0.5 & 0.244 & 0.325 & 0.431 \\
1.0 & 0.090 & 0.245 & 0.665 \\
2.0 & 0.016 & 0.117 & 0.867 \\
10.0 & $\approx 0$ & $\approx 0$ & $\approx 1$ \\
\hline
\end{tabular}
\end{center}

可以看到，随着 $\beta$ 增大，分布越来越集中在最大值上。

\section{在梯度老虎机算法中的应用}

\subsection{动作选择}

在梯度老虎机算法中，soft-max 用于将动作偏好 $H_t(a)$ 转换为选择概率：
\begin{equation}
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}
\end{equation}

\subsection{偏好的更新}

偏好值根据奖励进行更新：
\begin{align}
H_{t+1}(A_t) &= H_t(A_t) + \alpha[R_t - \bar{R}_t][1 - \pi_t(A_t)] \\
H_{t+1}(a) &= H_t(a) - \alpha[R_t - \bar{R}_t]\pi_t(a), \quad \forall a \neq A_t
\end{align}

\subsection{为什么使用 Soft-Max？}

\begin{enumerate}
    \item \textbf{平滑性}：提供平滑的概率分布，便于梯度优化
    \item \textbf{可微性}：所有概率都是可微的，支持基于梯度的学习
    \item \textbf{归一化}：自动保证概率和为 1
    \item \textbf{相对性}：只依赖于偏好值的相对大小，对平移不敏感
\end{enumerate}

\section{总结}

Soft-Max 分布是一个强大而灵活的工具，具有以下关键特性：

\begin{enumerate}
    \item \textbf{归一化}：自动将任意实数向量转换为概率分布
    \item \textbf{平移不变性}：只依赖于输入值的相对大小
    \item \textbf{可微性}：支持基于梯度的优化
    \item \textbf{数值稳定性}：可以通过减去最大值来避免溢出
    \item \textbf{灵活性}：通过温度参数可以控制分布的"尖锐程度"
\end{enumerate}

这些特性使得 soft-max 在机器学习、强化学习等领域得到了广泛应用。

\vspace{1cm}

\textbf{参考文献}：
\begin{itemize}
    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). Deep Learning. MIT Press.
    \item Sutton, R. S., \& Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition). MIT Press.
    \item Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
\end{itemize}

\end{document}


