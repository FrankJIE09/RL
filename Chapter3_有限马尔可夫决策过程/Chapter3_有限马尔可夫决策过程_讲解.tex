\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}

\geometry{margin=2.5cm}

\title{第3章：有限马尔可夫决策过程（Finite Markov Decision Processes）讲解}
\author{}
\date{}

\begin{document}

\maketitle

\section{章节概述}

第3章介绍了有限马尔可夫决策过程（MDP），这是本书其余部分试图解决的形式化问题。MDP 是强化学习的数学基础，为理解后续所有算法提供了理论框架。

\section{核心概念}

\subsection{智能体-环境接口}

\textbf{基本元素}：
\begin{itemize}
    \item \textbf{智能体（Agent）}：学习者和决策者
    \item \textbf{环境（Environment）}：智能体之外的一切
    \item \textbf{状态（State）}：$S_t \in \mathcal{S}$
    \item \textbf{动作（Action）}：$A_t \in \mathcal{A}(s)$
    \item \textbf{奖励（Reward）}：$R_{t+1} \in \mathcal{R}$
\end{itemize}

\textbf{交互序列}：
\begin{equation}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
\end{equation}

\textbf{关键点}：
\begin{itemize}
    \item 智能体-环境边界不是物理边界
    \item 边界代表智能体绝对控制的极限，而非知识的极限
    \item 奖励计算被认为是外部的，因为它定义了任务
\end{itemize}

\subsection{马尔可夫性质}

\textbf{定义}：状态必须包含过去智能体-环境交互的所有相关信息，这些信息对未来有影响。

\textbf{数学表达}：
\begin{equation}
p(s', r | s, a) = \Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
\end{equation}

\textbf{关键特性}：
\begin{itemize}
    \item 下一个状态和奖励的概率只依赖于当前状态和动作
    \item 不依赖于更早的状态和动作
    \item 这是对状态的限制，而非对决策过程的限制
\end{itemize}

\subsection{动态函数}

\textbf{四参数动态函数}：
\begin{equation}
p(s', r | s, a) = \Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
\end{equation}

\textbf{派生函数}：
\begin{itemize}
    \item \textbf{状态转移概率}：$p(s' | s, a) = \sum_r p(s', r | s, a)$
    \item \textbf{期望奖励}：$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a]$
    \item \textbf{状态-动作-下一状态期望奖励}：$r(s, a, s')$
\end{itemize}

\subsection{目标与奖励}

\textbf{奖励假设（Reward Hypothesis）}：
\begin{quote}
我们所说的目标和目的都可以很好地理解为最大化接收到的标量信号（称为奖励）的累积和的期望值。
\end{quote}

\textbf{关键原则}：
\begin{itemize}
    \item 奖励信号告诉智能体\textbf{要完成什么}，而不是\textbf{如何完成}
    \item 奖励应该反映真正的目标，而不是子目标
    \item 奖励设计是强化学习中最具挑战性的部分之一
\end{itemize}

\subsection{回报与回合}

\textbf{回报（Return）}：未来奖励的累积和

\textbf{两种任务类型}：

\subsubsection{回合任务（Episodic Tasks）}
\begin{equation}
G_t = R_{t+1} + R_{t+2} + \cdots + R_T
\end{equation}
\begin{itemize}
    \item 有明确的终止时间 $T$
    \item 自然分解为多个回合
    \item 每个回合在终端状态结束
\end{itemize}

\subsubsection{持续任务（Continuing Tasks）}
\begin{equation}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}
\begin{itemize}
    \item 没有明确的终止时间
    \item 使用\textbf{折扣因子} $\gamma \in [0, 1]$
    \item $\gamma = 0$：只关心即时奖励（短视）
    \item $\gamma \to 1$：更重视未来奖励（远视）
\end{itemize}

\textbf{回报的递归关系}：
\begin{equation}
G_t = R_{t+1} + \gamma G_{t+1}
\end{equation}

\subsection{策略与价值函数}

\subsubsection{策略（Policy）}

\textbf{定义}：从状态到选择每个可能动作的概率的映射
\begin{equation}
\pi(a | s) = \Pr\{A_t = a | S_t = s\}
\end{equation}

\subsubsection{状态价值函数}

\textbf{定义}：在策略 $\pi$ 下，状态 $s$ 的价值是期望回报
\begin{equation}
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s\right]
\end{equation}

\subsubsection{动作价值函数}

\textbf{定义}：在策略 $\pi$ 下，在状态 $s$ 采取动作 $a$ 的价值
\begin{equation}
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
\end{equation}

\subsubsection{贝尔曼方程}

\textbf{状态价值函数的贝尔曼方程}：
\begin{equation}
v_\pi(s) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\end{equation}

\textbf{动作价值函数的贝尔曼方程}：
\begin{equation}
q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a')\right]
\end{equation}

\textbf{关键特性}：
\begin{itemize}
    \item 价值函数是贝尔曼方程的唯一解
    \item 贝尔曼方程表达了状态价值与其后继状态价值之间的关系
    \item 这是强化学习算法的基础
\end{itemize}

\subsection{最优策略与最优价值函数}

\subsubsection{最优状态价值函数}

\textbf{定义}：
\begin{equation}
v_*(s) = \max_\pi v_\pi(s), \quad \text{对所有 } s \in \mathcal{S}
\end{equation}

\subsubsection{最优动作价值函数}

\textbf{定义}：
\begin{equation}
q_*(s, a) = \max_\pi q_\pi(s, a), \quad \text{对所有 } s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

\subsubsection{贝尔曼最优性方程}

\textbf{$v_*$ 的贝尔曼最优性方程}：
\begin{equation}
v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
\end{equation}

\textbf{$q_*$ 的贝尔曼最优性方程}：
\begin{equation}
q_*(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q_*(s', a')\right]
\end{equation}

\textbf{关键特性}：
\begin{itemize}
    \item 最优价值函数是唯一的
    \item 可能有多个最优策略，但它们共享相同的最优价值函数
    \item 任何相对于最优价值函数是贪婪的策略都是最优策略
\end{itemize}

\subsubsection{最优策略}

\textbf{从 $v_*$ 得到最优策略}：
\begin{itemize}
    \item 对每个状态 $s$，选择使 $q_*(s, a)$ 最大的动作
    \item 或者选择使 $\sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$ 最大的动作
\end{itemize}

\textbf{从 $q_*$ 得到最优策略}：
\begin{itemize}
    \item 对每个状态 $s$，选择使 $q_*(s, a)$ 最大的动作
    \item 不需要知道环境动态
\end{itemize}

\subsection{最优性与近似}

\textbf{现实限制}：
\begin{enumerate}
    \item \textbf{计算资源}：即使有完整模型，通常也无法计算最优策略
    \item \textbf{内存限制}：大状态空间需要函数近似
    \item \textbf{模型知识}：通常没有完整准确的环境模型
\end{enumerate}

\textbf{近似方法}：
\begin{itemize}
    \item \textbf{表格方法}：小状态空间，每个状态一个条目
    \item \textbf{函数近似}：大状态空间，使用参数化函数
\end{itemize}

\textbf{关键洞察}：
\begin{itemize}
    \item 在线学习的性质使得可以优先学习经常遇到的状态
    \item 这是强化学习与其他近似求解 MDP 方法的关键区别
\end{itemize}

\section{重要示例}

\subsection{示例 3.3：回收机器人}

\textbf{状态}：$\mathcal{S} = \{\text{high}, \text{low}\}$（电池电量）

\textbf{动作}：
\begin{itemize}
    \item $\mathcal{A}(\text{high}) = \{\text{search}, \text{wait}\}$
    \item $\mathcal{A}(\text{low}) = \{\text{search}, \text{wait}, \text{recharge}\}$
\end{itemize}

\textbf{奖励}：
\begin{itemize}
    \item 收集到罐子：$+1$
    \item 电池耗尽：$-3$
    \item 其他：$0$
\end{itemize}

\textbf{转移概率}：
\begin{itemize}
    \item 高电量搜索：以概率 $\alpha$ 保持高电量，以概率 $1-\alpha$ 变为低电量
    \item 低电量搜索：以概率 $\beta$ 保持低电量，以概率 $1-\beta$ 耗尽电池
\end{itemize}

\subsection{示例 3.5：网格世界}

\textbf{特点}：
\begin{itemize}
    \item 矩形网格，每个单元格是一个状态
    \item 四个动作：北、南、东、西
    \item 特殊状态 A 和 B 有特殊奖励和转移
\end{itemize}

\textbf{价值函数可视化}：
\begin{itemize}
    \item 显示了在等概率随机策略下的状态价值函数
    \item 展示了折扣奖励如何影响价值
\end{itemize}

\section{关键要点}

\begin{enumerate}
    \item \textbf{MDP 是强化学习的数学基础}
    \begin{itemize}
        \item 提供了形式化框架
        \item 为理论分析提供了基础
    \end{itemize}
    
    \item \textbf{马尔可夫性质是关键假设}
    \begin{itemize}
        \item 状态必须包含所有相关信息
        \item 使得问题可处理
    \end{itemize}
    
    \item \textbf{价值函数是核心概念}
    \begin{itemize}
        \item 状态价值函数 $v_\pi(s)$
        \item 动作价值函数 $q_\pi(s, a)$
        \item 最优价值函数 $v_*$ 和 $q_*$
    \end{itemize}
    
    \item \textbf{贝尔曼方程是算法基础}
    \begin{itemize}
        \item 表达了价值函数之间的递归关系
        \item 是动态规划和强化学习算法的核心
    \end{itemize}
    
    \item \textbf{最优性通常是理想}
    \begin{itemize}
        \item 实际中需要近似
        \item 在线学习提供了独特的近似机会
    \end{itemize}
\end{enumerate}

\section{练习题要点}

\begin{itemize}
    \item \textbf{练习 3.1-3.3}：理解 MDP 框架的灵活性和边界划分
    \item \textbf{练习 3.5-3.10}：掌握回报和折扣的计算
    \item \textbf{练习 3.11-3.19}：理解价值函数和贝尔曼方程
    \item \textbf{练习 3.20-3.29}：掌握最优价值函数和最优策略
\end{itemize}

\section{与后续章节的联系}

本章为后续所有章节奠定了基础：

\begin{itemize}
    \item \textbf{第4章}：动态规划（已知完整模型）
    \item \textbf{第5章}：蒙特卡洛方法（从经验中学习）
    \item \textbf{第6章}：时序差分学习（结合动态规划和蒙特卡洛）
    \item \textbf{第9-13章}：函数近似和策略梯度方法
\end{itemize}

\section{学习建议}

\begin{enumerate}
    \item \textbf{深入理解马尔可夫性质}
    \begin{itemize}
        \item 理解为什么需要这个假设
        \item 理解状态表示的重要性
    \end{itemize}
    
    \item \textbf{掌握价值函数的概念}
    \begin{itemize}
        \item 理解状态价值和动作价值的区别
        \item 理解它们之间的关系
    \end{itemize}
    
    \item \textbf{熟练使用贝尔曼方程}
    \begin{itemize}
        \item 理解递归关系的含义
        \item 能够推导和应用
    \end{itemize}
    
    \item \textbf{理解最优性}
    \begin{itemize}
        \item 理解最优价值函数的唯一性
        \item 理解如何从最优价值函数得到最优策略
    \end{itemize}
    
    \item \textbf{认识近似的必要性}
    \begin{itemize}
        \item 理解为什么需要近似
        \item 理解在线学习的优势
    \end{itemize}
\end{enumerate}

\vspace{1cm}

\textbf{参考文献}：Reinforcement Learning: An Introduction (2nd Edition), Chapter 3

\end{document}

