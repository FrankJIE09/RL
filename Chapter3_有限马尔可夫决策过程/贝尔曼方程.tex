\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{tikz}
\usepackage{booktabs}

\geometry{margin=2.5cm}

\title{贝尔曼方程详解}
\subtitle{强化学习中的核心方程}
\author{}
\date{}

\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\newtheorem{proposition}{命题}
\newtheorem{example}{示例}
\newtheorem{remark}{注记}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{引言}

贝尔曼方程（Bellman Equation）是强化学习中的核心数学工具，它建立了价值函数之间的递归关系。理解贝尔曼方程对于掌握强化学习算法至关重要。本文档将详细解释各种形式的贝尔曼方程，包括它们的推导、直观理解和应用。

\section{价值函数基础}

\subsection{状态价值函数}

\begin{definition}[状态价值函数]
状态 $s$ 在策略 $\pi$ 下的价值函数 $v_\pi(s)$ 定义为从状态 $s$ 开始，遵循策略 $\pi$ 的期望回报：
\begin{equation}
v_\pi(s) \doteq \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \Big| S_t = s\right]
\label{eq:state_value}
\end{equation}
其中：
\begin{itemize}
    \item $G_t$ 是从时刻 $t$ 开始的回报（return）
    \item $\gamma \in [0, 1]$ 是折扣因子（discount factor）
    \item $\mathbb{E}_\pi$ 表示在策略 $\pi$ 下的期望
\end{itemize}
\end{definition}

\subsection{动作价值函数}

\begin{definition}[动作价值函数]
状态 $s$ 下采取动作 $a$，然后遵循策略 $\pi$ 的动作价值函数 $q_\pi(s, a)$ 定义为：
\begin{equation}
q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \Big| S_t = s, A_t = a\right]
\label{eq:action_value}
\end{equation}
\end{definition}

\subsection{价值函数之间的关系}

状态价值函数和动作价值函数之间存在以下关系：

\begin{align}
v_\pi(s) &= \sum_{a \in \mathcal{A}(s)} \pi(a | s) q_\pi(s, a)
\label{eq:v_to_q}
\end{align}

\begin{align}
q_\pi(s, a) &= \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\label{eq:q_to_v}
\end{align}

\textbf{解释}：
\begin{itemize}
    \item 式 \eqref{eq:v_to_q}：状态价值是所有可能动作价值的加权平均，权重是策略概率
    \item 式 \eqref{eq:q_to_v}：动作价值是即时奖励加上折扣后的下一状态价值
\end{itemize}

\section{状态价值函数的贝尔曼方程}

\subsection{基本形式}

\begin{theorem}[状态价值函数的贝尔曼方程]
状态价值函数 $v_\pi(s)$ 满足以下递归关系：
\begin{equation}
v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\label{eq:bellman_v}
\end{equation}
对所有 $s \in \mathcal{S}$ 成立。
\end{theorem}

\subsection{推导过程}

\textbf{方法一：从定义直接推导}

从状态价值函数的定义出发：
\begin{align}
v_\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t = s]
\end{align}

将回报分解为即时奖励和未来回报：
\begin{align}
G_t &= R_{t+1} + \gamma G_{t+1}
\end{align}

因此：
\begin{align}
v_\pi(s) &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
\end{align}

现在需要计算这两个期望。首先，对动作和下一状态进行全概率展开：
\begin{align}
\mathbb{E}_\pi[R_{t+1} | S_t = s] &= \sum_{a \in \mathcal{A}(s)} \pi(a | s) \sum_{s', r} p(s', r | s, a) \cdot r
\end{align}

对于第二项，使用全期望公式：
\begin{align}
\mathbb{E}_\pi[G_{t+1} | S_t = s] &= \sum_{a \in \mathcal{A}(s)} \pi(a | s) \sum_{s', r} p(s', r | s, a) \mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s']
\end{align}

由于马尔可夫性质，给定 $S_{t+1} = s'$，$G_{t+1}$ 的分布只依赖于 $s'$，因此：
\begin{align}
\mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s'] &= \mathbb{E}_\pi[G_{t+1} | S_{t+1} = s'] = v_\pi(s')
\end{align}

综合以上结果：
\begin{align}
v_\pi(s) &= \sum_{a \in \mathcal{A}(s)} \pi(a | s) \sum_{s', r} p(s', r | s, a) \left[r + \gamma v_\pi(s')\right]
\end{align}

\textbf{方法二：使用价值函数关系}

从关系式 \eqref{eq:v_to_q} 和 \eqref{eq:q_to_v}：
\begin{align}
v_\pi(s) &= \sum_{a} \pi(a | s) q_\pi(s, a) \\
         &= \sum_{a} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\end{align}

\subsection{直观理解}

贝尔曼方程表达了以下重要思想：

\begin{enumerate}
    \item \textbf{递归结构}：当前状态的价值等于即时奖励加上未来状态的折扣价值
    
    \item \textbf{期望操作}：
    \begin{itemize}
        \item 外层求和：对所有可能的动作 $a$ 求期望（按策略 $\pi$ 加权）
        \item 内层求和：对所有可能的下一状态 $s'$ 和奖励 $r$ 求期望（按环境动态 $p$ 加权）
    \end{itemize}
    
    \item \textbf{时间一致性}：方程将"现在"和"未来"联系起来，体现了动态规划的核心思想
    
    \item \textbf{策略依赖性}：价值函数依赖于策略 $\pi$，不同策略产生不同的价值函数
\end{enumerate}

\subsection{矩阵形式}

如果状态空间有限，可以将贝尔曼方程写成矩阵形式。定义：
\begin{itemize}
    \item $\mathbf{v}_\pi$：状态价值向量，第 $i$ 个元素是 $v_\pi(s_i)$
    \item $\mathbf{r}_\pi$：期望奖励向量，第 $i$ 个元素是 $\sum_a \pi(a | s_i) \sum_{s', r} p(s', r | s_i, a) \cdot r$
    \item $\mathbf{P}_\pi$：状态转移概率矩阵，元素 $P_\pi[i, j] = \sum_a \pi(a | s_i) \sum_r p(s_j, r | s_i, a)$
\end{itemize}

则贝尔曼方程可以写成：
\begin{equation}
\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi
\label{eq:bellman_matrix}
\end{equation}

解这个线性方程组：
\begin{equation}
\mathbf{v}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi
\label{eq:bellman_solution}
\end{equation}

其中 $\mathbf{I}$ 是单位矩阵。

\section{动作价值函数的贝尔曼方程}

\subsection{基本形式}

\begin{theorem}[动作价值函数的贝尔曼方程]
动作价值函数 $q_\pi(s, a)$ 满足以下递归关系：
\begin{equation}
q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a')\right]
\label{eq:bellman_q}
\end{equation}
对所有 $s \in \mathcal{S}$ 和 $a \in \mathcal{A}(s)$ 成立。
\end{theorem}

\subsection{推导过程}

从动作价值函数的定义：
\begin{align}
q_\pi(s, a) &= \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \\
            &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]
\end{align}

分解为即时奖励和未来回报：
\begin{align}
q_\pi(s, a) &= \mathbb{E}_\pi[R_{t+1} | S_t = s, A_t = a] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a]
\end{align}

第一项是期望即时奖励：
\begin{align}
\mathbb{E}_\pi[R_{t+1} | S_t = s, A_t = a] &= \sum_{s', r} p(s', r | s, a) \cdot r
\end{align}

第二项需要展开到下一状态：
\begin{align}
\mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a] &= \sum_{s', r} p(s', r | s, a) \mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s', R_{t+1} = r]
\end{align}

由于马尔可夫性质，给定 $S_{t+1} = s'$，$G_{t+1}$ 的分布只依赖于 $s'$ 和后续的策略选择：
\begin{align}
\mathbb{E}_\pi[G_{t+1} | S_{t+1} = s'] &= \sum_{a'} \pi(a' | s') \mathbb{E}_\pi[G_{t+1} | S_{t+1} = s', A_{t+1} = a'] \\
                                       &= \sum_{a'} \pi(a' | s') q_\pi(s', a')
\end{align}

因此：
\begin{align}
q_\pi(s, a) &= \sum_{s', r} p(s', r | s, a) \left[r + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a')\right]
\end{align}

\subsection{与状态价值函数的关系}

动作价值函数的贝尔曼方程也可以从状态价值函数推导：

\begin{align}
q_\pi(s, a) &= \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \\
            &= \sum_{s', r} p(s', r | s, a) \left[r + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a')\right]
\end{align}

这展示了两种价值函数之间的紧密联系。

\section{最优贝尔曼方程}

\subsection{最优价值函数}

\begin{definition}[最优状态价值函数]
最优状态价值函数 $v_*(s)$ 定义为所有策略中状态 $s$ 的最大价值：
\begin{equation}
v_*(s) \doteq \max_\pi v_\pi(s)
\label{eq:optimal_v}
\end{equation}
\end{definition}

\begin{definition}[最优动作价值函数]
最优动作价值函数 $q_*(s, a)$ 定义为所有策略中状态 $s$ 下动作 $a$ 的最大价值：
\begin{equation}
q_*(s, a) \doteq \max_\pi q_\pi(s, a)
\label{eq:optimal_q}
\end{equation}
\end{definition}

\subsection{最优状态价值函数的贝尔曼方程}

\begin{theorem}[最优状态价值函数的贝尔曼方程]
最优状态价值函数 $v_*(s)$ 满足：
\begin{equation}
v_*(s) = \max_{a \in \mathcal{A}(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
\label{eq:bellman_optimal_v}
\end{equation}
\end{theorem}

\textbf{推导}：

最优价值函数是所有策略中的最大值，因此：
\begin{align}
v_*(s) &= \max_\pi v_\pi(s) \\
       &= \max_\pi \sum_{a} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\end{align}

对于最优策略，在状态 $s$ 下应该选择使期望回报最大的动作，因此：
\begin{align}
v_*(s) &= \max_{a \in \mathcal{A}(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
\end{align}

\textbf{关键区别}：
\begin{itemize}
    \item 普通贝尔曼方程：对动作求\textbf{加权平均}（按策略 $\pi$）
    \item 最优贝尔曼方程：对动作求\textbf{最大值}（选择最优动作）
\end{itemize}

\subsection{最优动作价值函数的贝尔曼方程}

\begin{theorem}[最优动作价值函数的贝尔曼方程]
最优动作价值函数 $q_*(s, a)$ 满足：
\begin{equation}
q_*(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q_*(s', a')\right]
\label{eq:bellman_optimal_q}
\end{equation}
\end{theorem}

\textbf{推导}：

\begin{align}
q_*(s, a) &= \max_\pi q_\pi(s, a) \\
          &= \max_\pi \sum_{s', r} p(s', r | s, a) \left[r + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a')\right]
\end{align}

对于最优策略，在下一状态 $s'$ 应该选择最优动作，因此：
\begin{align}
q_*(s, a) &= \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q_*(s', a')\right]
\end{align}

\textbf{关键区别}：
\begin{itemize}
    \item 普通贝尔曼方程：对下一状态的动作求\textbf{加权平均}（按策略 $\pi$）
    \item 最优贝尔曼方程：对下一状态的动作求\textbf{最大值}（选择最优动作）
\end{itemize}

\subsection{最优策略}

一旦得到最优价值函数，可以构造最优策略：

\begin{proposition}[贪婪策略]
定义策略 $\pi_*$ 为：
\begin{equation}
\pi_*(a | s) = \begin{cases}
1, & \text{如果 } a = \arg\max_{a'} q_*(s, a') \\
0, & \text{否则}
\end{cases}
\label{eq:greedy_policy}
\end{equation}
则 $\pi_*$ 是最优策略。
\end{proposition}

\textbf{证明思路}：
\begin{itemize}
    \item 如果 $v_{\pi_*}(s) = v_*(s)$ 对所有 $s$ 成立，则 $\pi_*$ 是最优的
    \item 由于 $\pi_*$ 在每一步都选择最优动作，其价值函数满足最优贝尔曼方程
    \item 因此 $v_{\pi_*}(s) = v_*(s)$
\end{itemize}

\section{贝尔曼方程的求解}

\subsection{解析解}

对于有限状态空间的MDP，如果已知环境动态 $p(s', r | s, a)$ 和策略 $\pi$，可以通过求解线性方程组得到价值函数。

从矩阵形式 \eqref{eq:bellman_matrix}：
\begin{equation}
\mathbf{v}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi
\end{equation}

\textbf{适用条件}：
\begin{itemize}
    \item 状态空间有限且较小
    \item 已知完整的环境模型
    \item 矩阵求逆计算可行
\end{itemize}

\subsection{迭代求解}

\textbf{价值迭代（Value Iteration）}：

对于最优价值函数，使用迭代更新：
\begin{equation}
v_{k+1}(s) = \max_{a \in \mathcal{A}(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]
\label{eq:value_iteration}
\end{equation}

\textbf{策略迭代（Policy Iteration）}：

交替进行策略评估和策略改进：

\begin{enumerate}
    \item \textbf{策略评估}：求解 $v_\pi$（通过迭代或解析）
    \begin{equation}
    v_\pi^{k+1}(s) = \sum_{a} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi^k(s')]
    \end{equation}
    
    \item \textbf{策略改进}：更新策略为贪婪策略
    \begin{equation}
    \pi'(a | s) = \begin{cases}
    1, & \text{如果 } a = \arg\max_{a'} \sum_{s', r} p(s', r | s, a') [r + \gamma v_\pi(s')] \\
    0, & \text{否则}
    \end{cases}
    \end{equation}
\end{enumerate}

\subsection{收敛性}

\begin{theorem}[价值迭代收敛性]
如果 $\gamma < 1$ 或所有策略都是适当的（proper），则价值迭代算法收敛到最优价值函数 $v_*$。
\end{theorem}

\textbf{证明思路}：
\begin{itemize}
    \item 贝尔曼算子是一个压缩映射（contraction mapping）
    \item 压缩映射有唯一不动点
    \item 价值迭代收敛到该不动点，即最优价值函数
\end{itemize}

\section{数值示例}

\subsection{简单网格世界}

考虑一个 3×3 网格世界：
\begin{itemize}
    \item 状态：9 个格子，编号为 $s_1$ 到 $s_9$
    \item 动作：上、下、左、右
    \item 目标：$s_9$ 是终止状态，到达获得奖励 $+10$
    \item 每步移动奖励：$-1$
    \item 折扣因子：$\gamma = 0.9$
\end{itemize}

\subsection{状态价值计算示例}

假设在状态 $s_5$（中心），采取动作"右"：

\begin{align}
q_\pi(s_5, \text{右}) &= \sum_{s', r} p(s', r | s_5, \text{右}) [r + \gamma v_\pi(s')] \\
                      &= p(s_6, -1 | s_5, \text{右}) \cdot [-1 + 0.9 \cdot v_\pi(s_6)] \\
                      &= 1.0 \cdot [-1 + 0.9 \cdot v_\pi(s_6)] \\
                      &= -1 + 0.9 \cdot v_\pi(s_6)
\end{align}

如果 $v_\pi(s_6) = 5$，则：
\begin{equation}
q_\pi(s_5, \text{右}) = -1 + 0.9 \times 5 = -1 + 4.5 = 3.5
\end{equation}

\subsection{完整的状态价值计算}

假设策略 $\pi$ 是均匀随机策略（每个动作概率 $0.25$），计算 $v_\pi(s_5)$：

\begin{align}
v_\pi(s_5) &= \sum_{a} \pi(a | s_5) \sum_{s', r} p(s', r | s_5, a) [r + \gamma v_\pi(s')] \\
           &= 0.25 \cdot q_\pi(s_5, \text{上}) + 0.25 \cdot q_\pi(s_5, \text{下}) \\
           &\quad + 0.25 \cdot q_\pi(s_5, \text{左}) + 0.25 \cdot q_\pi(s_5, \text{右})
\end{align}

假设已知：
\begin{itemize}
    \item $v_\pi(s_2) = 2$（上）
    \item $v_\pi(s_8) = 4$（下）
    \item $v_\pi(s_4) = 3$（左）
    \item $v_\pi(s_6) = 5$（右）
\end{itemize}

则：
\begin{align}
q_\pi(s_5, \text{上}) &= -1 + 0.9 \times 2 = 0.8 \\
q_\pi(s_5, \text{下}) &= -1 + 0.9 \times 4 = 2.6 \\
q_\pi(s_5, \text{左}) &= -1 + 0.9 \times 3 = 1.7 \\
q_\pi(s_5, \text{右}) &= -1 + 0.9 \times 5 = 3.5
\end{align}

因此：
\begin{equation}
v_\pi(s_5) = 0.25 \times (0.8 + 2.6 + 1.7 + 3.5) = 0.25 \times 8.6 = 2.15
\end{equation}

\subsection{最优价值计算示例}

使用最优贝尔曼方程计算 $v_*(s_5)$：

\begin{align}
v_*(s_5) &= \max_{a} \sum_{s', r} p(s', r | s_5, a) [r + \gamma v_*(s')] \\
         &= \max \{ q_*(s_5, \text{上}), q_*(s_5, \text{下}), q_*(s_5, \text{左}), q_*(s_5, \text{右}) \}
\end{align}

假设已知最优价值：
\begin{itemize}
    \item $v_*(s_2) = 3$
    \item $v_*(s_8) = 6$
    \item $v_*(s_4) = 4$
    \item $v_*(s_6) = 7$
\end{itemize}

则：
\begin{align}
q_*(s_5, \text{上}) &= -1 + 0.9 \times 3 = 1.7 \\
q_*(s_5, \text{下}) &= -1 + 0.9 \times 6 = 4.4 \\
q_*(s_5, \text{左}) &= -1 + 0.9 \times 4 = 2.6 \\
q_*(s_5, \text{右}) &= -1 + 0.9 \times 7 = 5.3
\end{align}

因此：
\begin{equation}
v_*(s_5) = \max \{1.7, 4.4, 2.6, 5.3\} = 5.3
\end{equation}

最优动作是"右"，这与直觉一致（向右移动更接近目标 $s_9$）。

\section{贝尔曼方程的应用}

\subsection{在动态规划中的应用}

贝尔曼方程是动态规划算法的理论基础：

\begin{itemize}
    \item \textbf{价值迭代}：直接使用最优贝尔曼方程迭代更新价值函数
    \item \textbf{策略迭代}：使用普通贝尔曼方程评估策略，然后改进策略
    \item \textbf{策略评估}：通过迭代求解贝尔曼方程得到策略的价值函数
\end{itemize}

\subsection{在时序差分学习中的应用}

时序差分（TD）学习使用贝尔曼方程的采样版本：

\begin{equation}
v_\pi(s) \approx r + \gamma v_\pi(s')
\end{equation}

这形成了TD更新的基础：
\begin{equation}
v(s) \leftarrow v(s) + \alpha [r + \gamma v(s') - v(s)]
\end{equation}

其中 $\alpha$ 是学习率。

\subsection{在Q学习中的应用}

Q学习使用最优动作价值函数的贝尔曼方程：

\begin{equation}
q_*(s, a) \approx r + \gamma \max_{a'} q_*(s', a')
\end{equation}

Q学习更新规则：
\begin{equation}
q(s, a) \leftarrow q(s, a) + \alpha \left[r + \gamma \max_{a'} q(s', a') - q(s, a)\right]
\end{equation}

\subsection{在深度强化学习中的应用}

深度Q网络（DQN）使用神经网络近似 $q_*(s, a)$，损失函数基于贝尔曼方程：

\begin{equation}
\mathcal{L} = \mathbb{E}\left[\left(r + \gamma \max_{a'} q(s', a'; \theta^-) - q(s, a; \theta)\right)^2\right]
\end{equation}

其中 $\theta$ 是网络参数，$\theta^-$ 是目标网络参数。

\section{贝尔曼方程的扩展}

\subsection{多步贝尔曼方程}

$n$ 步贝尔曼方程考虑 $n$ 步的未来：

\begin{equation}
v_\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n v_\pi(S_{t+n}) \Big| S_t = s\right]
\end{equation}

\subsection{平均奖励贝尔曼方程}

对于平均奖励设置（$\gamma = 1$ 且回报有界），贝尔曼方程变为：

\begin{equation}
v_\pi(s) = \sum_{a} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r - \rho_\pi + v_\pi(s')]
\end{equation}

其中 $\rho_\pi$ 是平均奖励。

\subsection{部分可观测MDP中的贝尔曼方程}

在POMDP中，价值函数依赖于信念状态（belief state）$b$：

\begin{equation}
v_\pi(b) = \sum_{a} \pi(a | b) \sum_{o, r} p(o, r | b, a) [r + \gamma v_\pi(b')]
\end{equation}

其中 $b'$ 是更新后的信念状态。

\section{直观理解与图示}

\subsection{贝尔曼方程的递归结构}

\begin{center}
\begin{tikzpicture}[node distance=2cm]
    \node (current) [rectangle, draw, text width=2.5cm, text centered] {当前状态\\$v_\pi(s)$};
    \node (action) [rectangle, draw, text width=2.5cm, text centered, below left=of current] {动作 $a$\\$\pi(a|s)$};
    \node (next) [rectangle, draw, text width=2.5cm, text centered, below right=of current] {下一状态\\$v_\pi(s')$};
    \node (reward) [rectangle, draw, text width=2.5cm, text centered, below=of action] {即时奖励\\$r$};
    
    \draw[->] (current) -- node[left] {策略选择} (action);
    \draw[->] (action) -- node[below] {环境动态} (next);
    \draw[->] (action) -- (reward);
    \draw[->] (reward) -- node[right] {$r + \gamma v_\pi(s')$} (next);
    \draw[->, dashed] (next) -- node[above, sloped] {递归} (current);
\end{tikzpicture}
\end{center}

\subsection{最优贝尔曼方程的决策结构}

\begin{center}
\begin{tikzpicture}[node distance=2cm]
    \node (current) [rectangle, draw, text width=2.5cm, text centered] {当前状态\\$v_*(s)$};
    \node (action1) [rectangle, draw, text width=1.5cm, text centered, below left=of current] {动作 $a_1$\\$q_*(s,a_1)$};
    \node (action2) [rectangle, draw, text width=1.5cm, text centered, below=of current] {动作 $a_2$\\$q_*(s,a_2)$};
    \node (action3) [rectangle, draw, text width=1.5cm, text centered, below right=of current] {动作 $a_3$\\$q_*(s,a_3)$};
    
    \draw[->] (current) -- (action1);
    \draw[->] (current) -- (action2);
    \draw[->] (current) -- (action3);
    \node [below=of action2, text width=3cm, text centered] {选择最大值\\$\max_a q_*(s,a)$};
\end{tikzpicture}
\end{center}

\section{常见误区与澄清}

\subsection{误区一：贝尔曼方程是优化问题}

\textbf{误区}：贝尔曼方程是一个需要优化的目标函数。

\textbf{澄清}：贝尔曼方程是一个\textbf{恒等式}，描述了价值函数必须满足的关系。它不是优化目标，而是价值函数的定义性质。

\subsection{误区二：最优贝尔曼方程总是有唯一解}

\textbf{误区}：最优贝尔曼方程总是有唯一的最优价值函数。

\textbf{澄清}：在有限MDP中，如果 $\gamma < 1$ 或所有策略都是适当的，则最优价值函数唯一。但在某些情况下（如周期性问题），可能需要额外的条件。

\subsection{误区三：贝尔曼方程只适用于离散状态空间}

\textbf{误区}：贝尔曼方程只能用于离散状态和动作空间。

\textbf{澄清}：贝尔曼方程适用于连续状态和动作空间，但求解方法不同（如函数逼近、神经网络等）。

\section{总结}

\subsection{核心要点}

\begin{enumerate}
    \item \textbf{递归关系}：贝尔曼方程建立了价值函数的递归关系，将当前价值与未来价值联系起来
    
    \item \textbf{策略依赖性}：普通贝尔曼方程依赖于策略 $\pi$，不同策略产生不同的价值函数
    
    \item \textbf{最优性}：最优贝尔曼方程通过最大化操作找到最优价值函数和最优策略
    
    \item \textbf{统一框架}：贝尔曼方程为各种强化学习算法提供了统一的理论基础
\end{enumerate}

\subsection{方程对比}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{方程类型} & \textbf{操作} & \textbf{用途} \\
\hline
状态价值贝尔曼方程 & 加权平均（策略） & 策略评估 \\
\hline
动作价值贝尔曼方程 & 加权平均（策略） & 策略评估 \\
\hline
最优状态价值方程 & 最大值（动作） & 价值迭代 \\
\hline
最优动作价值方程 & 最大值（下一动作） & Q学习 \\
\hline
\end{tabular}
\end{center}

\subsection{学习建议}

\begin{itemize}
    \item \textbf{理解递归结构}：掌握价值函数如何递归地依赖于未来状态
    
    \item \textbf{区分策略和最优}：理解普通贝尔曼方程和最优贝尔曼方程的区别
    
    \item \textbf{掌握推导}：能够从定义推导出贝尔曼方程
    
    \item \textbf{联系算法}：理解各种强化学习算法如何基于贝尔曼方程
    
    \item \textbf{实践应用}：通过数值示例加深理解
\end{itemize}

\vspace{1cm}

\textbf{参考文献}：
\begin{itemize}
    \item Sutton, R. S., \& Barto, A. G. (2018). \textit{Reinforcement Learning: An Introduction} (2nd Edition). MIT Press, Chapter 3.
    \item Bellman, R. (1957). \textit{Dynamic Programming}. Princeton University Press.
    \item Puterman, M. L. (2014). \textit{Markov Decision Processes: Discrete Stochastic Dynamic Programming}. John Wiley \& Sons.
\end{itemize}

\end{document}
