\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{tikz}

\geometry{margin=2.5cm}

\title{强化学习中的时间索引约定详解}
\subtitle{为什么是 $S_0, A_0, R_1, S_1, A_1, R_2, \ldots$ 而不是 $S_0, A_0, R_0$？}
\author{}
\date{}

\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\newtheorem{proposition}{命题}
\newtheorem{example}{示例}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{问题提出}

在强化学习中，智能体与环境的交互序列通常表示为：
\begin{equation}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
\label{eq:sequence}
\end{equation}

一个常见的问题是：\textbf{为什么奖励是 $R_1, R_2, R_3, \ldots$ 而不是 $R_0, R_1, R_2, \ldots$？}

本文档详细解释这个索引约定的原因和意义。

\section{标准序列表示}

\subsection{完整序列}

在马尔可夫决策过程（MDP）中，交互序列的标准表示是：
\begin{equation}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, S_3, A_3, R_4, \ldots
\end{equation}

\subsection{时间步的对应关系}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{时间步} & \textbf{状态} & \textbf{动作} & \textbf{奖励} \\
\hline
$t=0$ & $S_0$ & $A_0$ & - \\
$t=1$ & $S_1$ & $A_1$ & $R_1$ \\
$t=2$ & $S_2$ & $A_2$ & $R_2$ \\
$t=3$ & $S_3$ & $A_3$ & $R_3$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline
\end{tabular}
\end{center}

\textbf{关键观察}：
\begin{itemize}
    \item 在时间步 $t=0$，有状态 $S_0$ 和动作 $A_0$，但\textbf{没有奖励}
    \item 在时间步 $t=1$，有状态 $S_1$、动作 $A_1$ 和奖励 $R_1$
    \item 奖励 $R_1$ 是\textbf{由于}在时间步 $t=0$ 采取动作 $A_0$ 而产生的
\end{itemize}

\section{为什么使用 $R_{t+1}$ 而不是 $R_t$？}

\subsection{主要原因：强调联合确定}

\begin{proposition}[联合确定性质]
奖励 $R_{t+1}$ 和下一状态 $S_{t+1}$ 是\textbf{联合确定}的，它们都是由于在时间步 $t$ 采取动作 $A_t$ 而产生的。
\end{proposition}

\textbf{数学表达}：
\begin{equation}
p(s', r | s, a) = \Pr\{S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a\}
\end{equation}

\textbf{关键点}：
\begin{itemize}
    \item $R_{t+1}$ 和 $S_{t+1}$ 出现在\textbf{同一个时间步}（$t+1$）
    \item 它们都依赖于\textbf{同一个}状态-动作对 $(S_t, A_t)$
    \item 使用相同的下标 $t+1$ 强调了这种\textbf{同时性}和\textbf{关联性}
\end{itemize}

\subsection{时间线可视化}

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % 时间轴
    \draw[->] (0,0) -- (10,0);
    \node[below] at (0,0) {$t=0$};
    \node[below] at (2,0) {$t=1$};
    \node[below] at (4,0) {$t=2$};
    \node[below] at (6,0) {$t=3$};
    
    % 状态
    \node[above] at (0,0.5) {$S_0$};
    \node[above] at (2,0.5) {$S_1$};
    \node[above] at (4,0.5) {$S_2$};
    \node[above] at (6,0.5) {$S_3$};
    
    % 动作
    \node[above] at (0,1) {$A_0$};
    \node[above] at (2,1) {$A_1$};
    \node[above] at (4,1) {$A_2$};
    \node[above] at (6,1) {$A_3$};
    
    % 奖励
    \node[below] at (2,-0.5) {$R_1$};
    \node[below] at (4,-0.5) {$R_2$};
    \node[below] at (6,-0.5) {$R_3$};
    
    % 箭头表示因果关系
    \draw[->,red,thick] (0.5,1) -- (1.5,0.5);
    \draw[->,red,thick] (0.5,1) -- (1.5,-0.5);
    \node[red,right] at (1,0.2) {导致};
    
    \draw[->,blue,thick] (2.5,1) -- (3.5,0.5);
    \draw[->,blue,thick] (2.5,1) -- (3.5,-0.5);
    \node[blue,right] at (3,0.2) {导致};
\end{tikzpicture}
\end{center}

\textbf{解释}：
\begin{itemize}
    \item 在时间 $t=0$，智能体观察到 $S_0$，选择 $A_0$
    \item 在时间 $t=1$，环境\textbf{同时}返回 $R_1$ 和 $S_1$
    \item $R_1$ 和 $S_1$ 都是 $A_0$ 的结果，因此使用相同的下标 $1$
\end{itemize}

\subsection{因果关系的清晰表达}

使用 $R_{t+1}$ 的约定使得因果关系更加清晰：

\begin{center}
\begin{tabular}{|c|l|}
\hline
\textbf{时间步 $t$} & \textbf{发生的事件} \\
\hline
$t$ & 智能体观察到 $S_t$，选择 $A_t$ \\
\hline
$t+1$ & 环境返回 $R_{t+1}$ 和 $S_{t+1}$ \\
      & （两者都是 $A_t$ 的结果） \\
\hline
\end{tabular}
\end{center}

\textbf{如果使用 $R_t$ 会怎样？}

假设使用 $R_t$ 表示在时间 $t$ 收到的奖励：
\begin{equation}
S_0, A_0, R_0, S_1, A_1, R_1, S_2, A_2, R_2, \ldots
\end{equation}

\textbf{问题}：
\begin{itemize}
    \item $R_0$ 似乎是在时间 $t=0$ 收到的，但此时还没有采取任何动作
    \item $R_1$ 和 $S_1$ 的下标不一致，但它们实际上是同时产生的
    \item 因果关系不够清晰：$R_1$ 看起来像是 $A_1$ 的结果，而不是 $A_0$ 的结果
\end{itemize}

\section{动态函数的表示}

\subsection{四参数动态函数}

在 MDP 中，动态函数定义为：
\begin{equation}
p(s', r | s, a) = \Pr\{S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a\}
\label{eq:dynamics}
\end{equation}

\textbf{关键观察}：
\begin{itemize}
    \item 条件部分：$S_t = s, A_t = a$（时间 $t$ 的状态和动作）
    \item 结果部分：$S_{t+1} = s', R_{t+1} = r$（时间 $t+1$ 的状态和奖励）
    \item $S_{t+1}$ 和 $R_{t+1}$ 使用相同的下标，强调它们同时产生
\end{itemize}

\subsection{如果使用 $R_t$ 会怎样？}

如果使用 $R_t$ 表示奖励，动态函数会变成：
\begin{equation}
p(s', r | s, a) = \Pr\{S_{t+1} = s', R_t = r | S_t = s, A_t = a\}
\end{equation}

\textbf{问题}：
\begin{itemize}
    \item $R_t$ 和 $S_{t+1}$ 的下标不一致（$t$ vs $t+1$）
    \item 这暗示它们可能不在同一时间步，但实际上它们是同时产生的
    \item 数学表达不够直观
\end{itemize}

\section{回报（Return）的计算}

\subsection{折扣回报}

在强化学习中，回报定义为：
\begin{equation}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

\textbf{使用 $R_{t+1}$ 的优势}：
\begin{itemize}
    \item 从时间 $t$ 开始，第一个奖励是 $R_{t+1}$（下一个时间步）
    \item 索引清晰：$R_{t+1}, R_{t+2}, R_{t+3}, \ldots$
    \item 与状态序列 $S_t, S_{t+1}, S_{t+2}, \ldots$ 对应
\end{itemize}

\subsection{回报的递归关系}

回报的一个重要性质是递归关系：
\begin{equation}
G_t = R_{t+1} + \gamma G_{t+1}
\label{eq:return_recursive}
\end{equation}

\textbf{解释}：
\begin{itemize}
    \item 在时间 $t$ 的回报 = 下一个奖励 $R_{t+1}$ + 折扣后的未来回报 $\gamma G_{t+1}$
    \item 使用 $R_{t+1}$ 使得这个关系非常自然
    \item 如果使用 $R_t$，关系会变成 $G_t = R_t + \gamma G_{t+1}$，但 $R_t$ 是"过去"的奖励，逻辑上不够清晰
\end{itemize}

\section{价值函数的定义}

\subsection{状态价值函数}

状态价值函数定义为：
\begin{equation}
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s\right]
\end{equation}

\textbf{使用 $R_{t+1}$ 的优势}：
\begin{itemize}
    \item 从状态 $S_t = s$ 开始，第一个奖励是 $R_{t+1}$
    \item 索引从 $k=0$ 开始，对应 $R_{t+0+1} = R_{t+1}$
    \item 数学表达简洁清晰
\end{itemize}

\subsection{动作价值函数}

动作价值函数定义为：
\begin{equation}
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s, A_t = a\right]
\end{equation}

\textbf{关键点}：
\begin{itemize}
    \item 给定 $S_t = s, A_t = a$，第一个奖励是 $R_{t+1}$
    \item 这强调了 $R_{t+1}$ 是 $A_t$ 的结果
\end{itemize}

\section{贝尔曼方程}

\subsection{状态价值函数的贝尔曼方程}

\begin{equation}
v_\pi(s) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\end{equation}

\textbf{解释}：
\begin{itemize}
    \item $p(s', r | s, a)$ 表示：在状态 $s$ 采取动作 $a$，得到下一状态 $s'$ 和奖励 $r$ 的概率
    \item 奖励 $r$ 对应 $R_{t+1}$，下一状态 $s'$ 对应 $S_{t+1}$
    \item 两者在动态函数中使用相同的条件，强调了它们的联合性
\end{itemize}

\section{实际例子}

\subsection{示例：网格世界}

考虑一个简单的网格世界：
\begin{itemize}
    \item 在时间 $t=0$：智能体在位置 $S_0 = (0, 0)$，选择动作 $A_0 = \text{右}$
    \item 在时间 $t=1$：智能体移动到 $S_1 = (1, 0)$，收到奖励 $R_1 = -1$（移动成本）
    \item 在时间 $t=1$：智能体选择动作 $A_1 = \text{上}$
    \item 在时间 $t=2$：智能体移动到 $S_2 = (1, 1)$，收到奖励 $R_2 = +10$（到达目标）
\end{itemize}

\textbf{序列}：$S_0, A_0, R_1, S_1, A_1, R_2, S_2, \ldots$

\textbf{关键观察}：
\begin{itemize}
    \item $R_1$ 是 $A_0$（向右移动）的结果
    \item $R_1$ 和 $S_1$ 同时出现（都在时间 $t=1$）
    \item 使用 $R_1$ 而不是 $R_0$ 使得这个关系清晰
\end{itemize}

\section{其他约定}

\subsection{文献中的不同约定}

\begin{quote}
\textit{"We use $R_{t+1}$ instead of $R_t$ to denote the reward due to $A_t$ because it emphasizes that the next reward and next state, $R_{t+1}$ and $S_{t+1}$, are jointly determined. Unfortunately, both conventions are widely used in the literature."}
\end{quote}

\textbf{两种约定}：

\textbf{约定 1}（本书使用）：
\begin{equation}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
\end{equation}
\begin{itemize}
    \item 优点：强调 $R_{t+1}$ 和 $S_{t+1}$ 的联合性
    \item 优点：回报计算更直观
    \item 缺点：奖励索引比状态/动作索引"滞后"1
\end{itemize}

\textbf{约定 2}（某些文献使用）：
\begin{equation}
S_0, A_0, R_0, S_1, A_1, R_1, S_2, A_2, R_2, \ldots
\end{equation}
\begin{itemize}
    \item 优点：所有变量的索引"对齐"
    \item 缺点：$R_0$ 的含义不清晰（在时间 $t=0$ 还没有动作）
    \item 缺点：$R_t$ 和 $S_{t+1}$ 的下标不一致，但它们同时产生
\end{itemize}

\subsection{为什么本书选择约定 1？}

\begin{enumerate}
    \item \textbf{强调因果关系}：$R_{t+1}$ 明确表示是 $A_t$ 的结果
    \item \textbf{强调联合性}：$R_{t+1}$ 和 $S_{t+1}$ 使用相同下标，强调同时产生
    \item \textbf{数学表达清晰}：动态函数 $p(s', r | s, a)$ 中，$r$ 和 $s'$ 对应 $R_{t+1}$ 和 $S_{t+1}$
    \item \textbf{回报计算直观}：$G_t = R_{t+1} + \gamma G_{t+1}$ 非常自然
\end{enumerate}

\section{常见误解}

\subsection{误解 1：$R_1$ 是"第一个奖励"}

\textbf{误解}：$R_1$ 是序列中的第一个奖励，所以应该从 $R_0$ 开始。

\textbf{纠正}：
\begin{itemize}
    \item $R_1$ 确实是序列中\textbf{出现的}第一个奖励
    \item 但它是在时间 $t=1$ 收到的，是 $A_0$ 的结果
    \item 在时间 $t=0$ 没有奖励，因为还没有采取任何动作
\end{itemize}

\subsection{误解 2：索引应该对齐}

\textbf{误解}：所有变量应该使用相同的索引，即 $S_t, A_t, R_t$。

\textbf{纠正}：
\begin{itemize}
    \item 虽然索引对齐看起来更"整齐"，但会失去重要的语义信息
    \item $R_{t+1}$ 的约定强调了它是"下一个"时间步的奖励
    \item 这更符合因果关系的逻辑
\end{itemize}

\subsection{误解 3：$R_0$ 可以表示初始奖励}

\textbf{误解}：$R_0$ 可以表示环境的初始奖励或起始奖励。

\textbf{纠正}：
\begin{itemize}
    \item 在标准 MDP 框架中，奖励是动作的结果
    \item 在时间 $t=0$，还没有采取动作，因此不应该有奖励
    \item 如果确实有初始奖励，应该明确说明，而不是使用 $R_0$
\end{itemize}

\section{总结}

\subsection{关键要点}

\begin{enumerate}
    \item \textbf{序列表示}：$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$
    
    \item \textbf{使用 $R_{t+1}$ 的原因}：
    \begin{itemize}
        \item 强调 $R_{t+1}$ 和 $S_{t+1}$ 是联合确定的
        \item 强调 $R_{t+1}$ 是 $A_t$ 的结果
        \item 使得动态函数和回报计算更直观
    \end{itemize}
    
    \item \textbf{时间对应关系}：
    \begin{itemize}
        \item 时间 $t$：观察 $S_t$，选择 $A_t$
        \item 时间 $t+1$：收到 $R_{t+1}$ 和 $S_{t+1}$（两者都是 $A_t$ 的结果）
    \end{itemize}
    
    \item \textbf{数学优势}：
    \begin{itemize}
        \item 动态函数：$p(s', r | s, a) = \Pr\{S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a\}$
        \item 回报递归：$G_t = R_{t+1} + \gamma G_{t+1}$
        \item 价值函数：$v_\pi(s) = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$
    \end{itemize}
\end{enumerate}

\subsection{记忆技巧}

\begin{itemize}
    \item \textbf{因果关系}：动作 $A_t$ \textbf{导致}奖励 $R_{t+1}$ 和状态 $S_{t+1}$
    \item \textbf{同时性}：$R_{t+1}$ 和 $S_{t+1}$ 使用相同下标，因为它们同时产生
    \item \textbf{未来性}：$R_{t+1}$ 是"下一个"时间步的奖励，所以索引是 $t+1$
\end{itemize}

\vspace{1cm}

\textbf{参考文献}：
\begin{itemize}
    \item Sutton, R. S., \& Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition). MIT Press, Chapter 3, Section 3.1.
    \item 书中明确说明："We use $R_{t+1}$ instead of $R_t$ to denote the reward due to $A_t$ because it emphasizes that the next reward and next state, $R_{t+1}$ and $S_{t+1}$, are jointly determined."
\end{itemize}

\end{document}


